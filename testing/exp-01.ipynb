{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ENT_TYPE': 'PERSON', 'OP': '{2}'}, {'LEMMA': 'run'}]\n",
      "[[(8128502578493265141, 0, 3)], []]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def create_pattern(text_input):\n",
    "    parts = text_input.split()\n",
    "    base_patterns = [\n",
    "        \"orth\", \"text\", \"norm\", \"lower\", \"lemma\"\n",
    "    ]\n",
    "\n",
    "    grammar_patterns = [\n",
    "        \"pos\", \"tag\", \"morph\", \"dep\", \"shape\", \"ent_type\", \"ent_iob\", \"ent_id\", \"ent_kb_id\"\n",
    "\n",
    "    ]\n",
    "\n",
    "    op = \"op\"\n",
    "\n",
    "    op_patterns = [\"!\", \"?\", \"+\", \"*\", \"{n}\", \"{n,m}\", \"{n,}\", \"{,m}\"]\n",
    "    \n",
    "    full_sequence = []\n",
    "\n",
    "    for part in parts:\n",
    "        if \"(\" in part and \")\" in part:  # Check format is correct\n",
    "            token, rules = part[:-1].split(\"(\")  # Remove the closing parenthesis while splitting\n",
    "            rules = rules.split(\"|\")\n",
    "            token_attributes = {}\n",
    "\n",
    "            for rule in rules:\n",
    "                rule_key = rule.lower()\n",
    "                if rule_key in base_patterns:\n",
    "                    if rule_key == \"lower\":\n",
    "                        token_attributes[rule_key.upper()] = token.lower()\n",
    "                    else:\n",
    "                        token_attributes[rule_key.upper()] = token\n",
    "                if rule_key.split(\"=\")[0] in grammar_patterns:\n",
    "                    key, value = rule.split(\"=\")\n",
    "                    key = key.upper()\n",
    "                    token_attributes[key.upper()] = value.upper()\n",
    "                if rule_key.split(\"=\")[0] == \"op\":\n",
    "                    key, value = rule_key.split(\"=\")\n",
    "                    token_attributes[key.upper()] = value.upper()\n",
    "\n",
    "            if token_attributes:\n",
    "                full_sequence.append(token_attributes)\n",
    "\n",
    "        else:\n",
    "            print(f\"Error in part format: {part}\")\n",
    "\n",
    "    return full_sequence\n",
    "\n",
    "def build_matcher(nlp, patterns):\n",
    "    from spacy.matcher import Matcher\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"PATTERN_NAME\", [patterns])\n",
    "    return matcher\n",
    "\n",
    "def query_docs(docs, matcher):\n",
    "    matches = []\n",
    "    for doc in docs:\n",
    "        matches.append(matcher(doc))\n",
    "    return matches\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Prepare documents\n",
    "texts = [\"John Smith ran fast.\", \"But Joe ran faster.\"]\n",
    "docs = list(nlp.pipe(texts))\n",
    "\n",
    "pattern = \"(ent_type=person|op={2}) run(lemma)\"\n",
    "# Setup matcher and query documents\n",
    "pattern = create_pattern(pattern)\n",
    "print(pattern)\n",
    "matcher = build_matcher(nlp, pattern)\n",
    "matched_docs = query_docs(docs, matcher)\n",
    "print(matched_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ENT_TYPE': 'PERSON', 'OP': '{3}'}, {'LEMMA': 'run'}, {'POS': 'ADV'}]\n",
      "[[], [], [(8128502578493265141, 0, 5)]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "class SpacyEx:\n",
    "    def __init__(self, nlp):\n",
    "        \"\"\"\n",
    "        Initialize the SpacyMatcher with a spaCy language model.\n",
    "\n",
    "        Args:\n",
    "        nlp (spacy.Language): A spaCy language model.\n",
    "        \"\"\"\n",
    "        self.nlp = nlp\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    def create_pattern(self, text_input):\n",
    "        \"\"\"\n",
    "        Process the input string to generate patterns for the spaCy Matcher.\n",
    "\n",
    "        Args:\n",
    "        text_input (str): The input string defining the patterns.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of dictionaries containing token attributes for matching.\n",
    "        \"\"\"\n",
    "        base_patterns = [\"orth\", \"text\", \"norm\", \"lower\", \"lemma\"]\n",
    "        grammar_patterns = [\n",
    "            \"pos\", \"tag\", \"morph\", \"dep\", \"shape\", \"ent_type\", \"ent_iob\", \"ent_id\", \"ent_kb_id\"\n",
    "        ]\n",
    "        full_sequence = []\n",
    "\n",
    "        for part in text_input.split():\n",
    "            if \"(\" in part and \")\" in part:\n",
    "                token, rules = part[:-1].split(\"(\")\n",
    "                rules = rules.split(\"|\")\n",
    "                token_attributes = {}\n",
    "\n",
    "                for rule in rules:\n",
    "                    key, _, value = rule.partition(\"=\")\n",
    "                    key_lower = key.lower()\n",
    "\n",
    "                    if key_lower in base_patterns:\n",
    "                        token_attributes[key.upper()] = token.lower() if key_lower == \"lower\" else token\n",
    "\n",
    "                    elif key_lower in grammar_patterns:\n",
    "                        token_attributes[key.upper()] = value.upper()\n",
    "\n",
    "                    elif key_lower == \"op\":\n",
    "                        token_attributes[key.upper()] = value\n",
    "\n",
    "                if token_attributes:\n",
    "                    full_sequence.append(token_attributes)\n",
    "            else:\n",
    "                print(f\"Error in part format: {part}\")\n",
    "\n",
    "        return full_sequence\n",
    "\n",
    "    def add_patterns(self, pattern_name, patterns):\n",
    "        \"\"\"\n",
    "        Add the specified patterns to the matcher.\n",
    "\n",
    "        Args:\n",
    "        pattern_name (str): The identifier for these patterns.\n",
    "        patterns (list): The patterns to add to the matcher.\n",
    "        \"\"\"\n",
    "        self.matcher.add(pattern_name, [patterns])\n",
    "\n",
    "    def query_docs(self, docs):\n",
    "        \"\"\"\n",
    "        Match patterns against a sequence of documents.\n",
    "\n",
    "        Args:\n",
    "        docs (Iterable[spacy.tokens.Doc]): Documents to match against.\n",
    "\n",
    "        Returns:\n",
    "        list: Match results for each document.\n",
    "        \"\"\"\n",
    "        matches = []\n",
    "        for doc in docs:\n",
    "            matches.append(self.matcher(doc))\n",
    "        return matches\n",
    "\n",
    "# Usage example\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher_tool = SpacyMatcher(nlp)\n",
    "pattern = \"(ent_type=person|op={3}) run(lemma) fast(pos=ADV)\"\n",
    "patterns = matcher_tool.create_pattern(pattern)\n",
    "print(patterns)\n",
    "\n",
    "matcher_tool.add_patterns(\"PATTERN_NAME\", patterns)\n",
    "texts = [\"John Smith ran fast.\", \"But Joe ran faster.\", \"John Jacob Smith ran fast.\"]\n",
    "docs = list(nlp.pipe(texts))\n",
    "matches = matcher_tool.query_docs(docs)\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holocaust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
